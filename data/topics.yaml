- title: "Gender Bias in AI"
  description: "LLMs may reinforce outdated gender stereotypes."
  case_study: "Male profiles more likely suggest 'CEO', female profiles 'nurse'."
  link: "https://www.example.org/gender-bias"
  hashtags:
    - "#AIethics"
    - "#GenderBias"
    - "#FairnessInAI"
  image_prompt: "A robot assigning stereotypical job roles based on gender"
  content: |
    LLMs must be evaluated for gender fairness. 
    Let's raise awareness about biases and work toward inclusive AI.

- title: "Racial Bias in Predictive Policing"
  description: "AI trained on historical data may amplify racial profiling."
  case_study: "Predictive policing flagged people of color at higher risk."
  link: "https://www.example.org/racial-bias"
  hashtags:
    - "#ResponsibleAI"
    - "#BiasInAI"
    - "#AIJustice"
  image_prompt: "A predictive map showing racial bias in risk zones"
  content: |
    Algorithms in law enforcement must be transparent and just.
    We need fairness audits and community oversight.

- title: "Algorithmic Fairness in Recruitment"
  description: "Bias in AI recruitment tools can lead to unfair hiring decisions."
  case_study: "AI screening systems have been shown to prefer certain demographic traits over others."
  link: "https://www.example.org/ai-hiring-bias"
  hashtags:
    - "#FairHiring"
    - "#AIEthics"
    - "#InclusiveTech"
  image_prompt: "An abstract AI hiring dashboard with diverse candidate profiles equally highlighted"
  content: |
    Algorithmic hiring tools are increasingly used in recruitment processes.
    But without oversight, these systems can unintentionally favor or exclude candidates based on biased data.
    To promote fairness:
     - Evaluate datasets for representation across demographics.
     - Regularly audit hiring algorithms for bias.
     - Ensure human oversight in final decisions.
    What best practices are you seeing in AI-based hiring systems?