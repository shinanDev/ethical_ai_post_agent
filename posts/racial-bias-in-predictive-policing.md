Data is not just neutral numbers; it carries the weight of our socio-cultural biases. Recently, AI systems used for predictive policing have sparked significant controversy. These systems are trained on historical crime data, which unfortunately, often reflect racial biases. The real-world impact? In Oakland, USA, an AI system disproportionately targeted Black neighborhoods for police patrols, echoing past patterns of racial profiling. This is not a case of AI gone wrong, but a mirror to our societal biases embedded in the data it learns from. As we strive for a future powered by AI, how can we ensure fairness and justice in our systems? I invite your thoughts on strategies to mitigate racial bias in predictive AI systems.